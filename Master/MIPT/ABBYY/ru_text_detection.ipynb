{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вступительное задание на кафедру компании ABBYY\n",
    "## Андрей Сонин\n",
    "\n",
    "### Условие\n",
    "\n",
    "Представим, что вы &mdash; разработчик в отделе машинного обучения и перед вами поставлена задача разработать модуль автоматического обнаружения русскоязычных текстов. На вход подаётся большой набор документов, среди которых нужно обнаружить тексты, содержащие значительные фрагменты на русском языке. [Пример входных данных](https://drive.google.com/file/d/1kVY4C-taT1h8XBvrUyHCJoLWKmXYWKzU/view?usp=sharing)\n",
    " \n",
    "В компании нет обучающих данных, но вы можете собрать необходимые данные самостоятельно, разрешается использовать любые данные для обучения, готовые модели и программные средства, например, https://www.sketchengine.eu/guide/create-a-corpus-from-the-web.\n",
    " \n",
    "Требуется реализовать функцию predict, принимающую путь к папке, в которой содержатся тексты, сохраненные в формате .txt (utf-8). В каждом файле содержится ровно один текст. Функция генерирует в заданной папке файл с предсказаниями prediction.csv в формате (filename;answer), где answer &mdash; 1 (содержит русский язык) или 0 (не содержит русского языка).\n",
    " \n",
    "Также реализуйте функцию predict_once, которая принимает строку с текстом и выводит ответ в стандартный вывод.\n",
    " \n",
    "Обратите внимание, что задача не является контестом, в качестве решения нужно предоставить код на Python (можно Jupyter) или C++. Другой язык программирования разрешается использовать только по предварительной договоренности. Решение должно содержать хотя бы минимальные тесты, оценку качества и анализ ошибок.\n",
    "\n",
    "## Системные требования\n",
    "- Python: >= 3.7\n",
    "- ОС: `Linux/macOS`\n",
    "\n",
    "## Предварительные приготовления\n",
    "### Загрузка дополнительных библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.util import find_spec\n",
    "from os import system\n",
    "\n",
    "\n",
    "def install_package(name: str,\n",
    "                    *,\n",
    "                    conda_suffix: str = '',\n",
    "                    pip_suffix: str = '') -> None:\n",
    "    \"\"\"Checks if a module is downloaded.\n",
    "    If not, tries to install it first with 'conda', then, if unsuccessful, with 'pip'.\n",
    "\n",
    "    Args:\n",
    "        name:          package name\n",
    "        conda_suffix:  additional part of 'conda' installation command\n",
    "        pip_suffix:    additional part of 'pip' installation command\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if (\n",
    "        not find_spec(name)\n",
    "        and system(f'conda install -y {name} {conda_suffix}')\n",
    "        and system(f'pip install {name} {pip_suffix}')\n",
    "    ):\n",
    "        raise RuntimeError(f\"Cannot install '{name}' module\")\n",
    "\n",
    "\n",
    "install_package('nltk')\n",
    "install_package('langdetect', conda_suffix='-c conda-forge')\n",
    "install_package('requests')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка примера входных данных из Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# >>> Google drive downloading script >>>\n",
    "from os import PathLike\n",
    "from os.path import isdir\n",
    "from typing import Union\n",
    "from requests import Session, Response\n",
    "\n",
    "\n",
    "def download_file_from_google_drive(google_id: str,\n",
    "                                    destination: Union[PathLike, str]) -> None:\n",
    "    URL = 'https://docs.google.com/uc?export=download'\n",
    "\n",
    "    session = Session()\n",
    "\n",
    "    response = session.get(URL, params={'id': google_id}, stream=True)\n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        response = session.get(\n",
    "            URL,\n",
    "            params={'id': google_id, 'confirm': token},\n",
    "            stream=True\n",
    "        )\n",
    "\n",
    "    save_response_content(response, destination)    \n",
    "\n",
    "\n",
    "def get_confirm_token(response: Response) -> None:\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "    return None\n",
    "\n",
    "\n",
    "def save_response_content(response: Response,\n",
    "                          destination: Union[PathLike, str],\n",
    "                          *,\n",
    "                          chunk_size: int = 100_000_000) -> None:\n",
    "    with open(destination, 'wb') as f:\n",
    "        for chunk in response.iter_content(chunk_size):\n",
    "            if chunk:  # filters out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "\n",
    "\n",
    "if not isdir('data'):\n",
    "    download_file_from_google_drive('1kVY4C-taT1h8XBvrUyHCJoLWKmXYWKzU', 'data.zip')\n",
    "    !unzip -o data.zip > /dev/null && rm data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подключение всех необходимых библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/asonin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from glob import iglob\n",
    "from itertools import compress\n",
    "from collections import Counter\n",
    "from pickle import dump, load\n",
    "from operator import itemgetter\n",
    "from csv import reader as csv_reader\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from os import PathLike, makedirs\n",
    "from os.path import getsize, basename, isfile, join as join_path\n",
    "from pathlib import Path\n",
    "from typing import Iterable, Iterator, Sequence, Callable, TypeVar\n",
    "\n",
    "import nltk\n",
    "from langdetect import detect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "from nltk import sent_tokenize, WordPunctTokenizer\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "T_co = TypeVar('T_co', covariant=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Основной код\n",
    "- Чтобы иметь дополнительные гарантии в корректности питоновского кода, были расставлены аннотации типов, а код проверен с помощью статического анализатора `MyPy`.\n",
    "\n",
    "- Для идентификации языка текста был использован наивный байесовский классификатор [`langdetect`](https://github.com/Mimino666/langdetect), обученный на 55 языках, включая русский.\n",
    "\n",
    "- Решение о наличии значительных фрагментов русского языка в тексте основывается на предположении о том, что каждое конкретное предложение в тексте написано на определённом языке. Для каждого предложения `langdetect` определяет его язык, и если доля предложений на русском превышает фиксированный порог, то считается, что данный текст содержит значительные фрагменты русского текста.\n",
    "\n",
    "- Для уменьшения времени детектирования функция `predict` выполнена параллельной."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>> Auxiliary utils >>>\n",
    "\n",
    "def split(iterable: Iterable[T_co],\n",
    "          n: int,\n",
    "          *,\n",
    "          collector: Callable[[Iterable[T_co]], Sequence[T_co]] = tuple) -> Iterator[Sequence[T_co]]:\n",
    "    \"\"\"Splits an iterable into 'n' equal pieces, if possible.\n",
    "    If not, the first few pieces will be 1 longer than the last ones.\n",
    "\n",
    "    >>> tuple(split(range(10), 3))\n",
    "    ((0, 1, 2, 3), (4, 5, 6), (7, 8, 9))\n",
    "\n",
    "    >>> list(split(range(7), 4))\n",
    "    [(0, 1), (2, 3), (4, 5), (6,)]\n",
    "\n",
    "    >>> list(split(range(7), 4, collector=list))\n",
    "    [[0, 1], [2, 3], [4, 5], [6]]\n",
    "\n",
    "    >>> list(split(range(3), 5, collector=list))\n",
    "    [[0], [1], [2], [], []]\n",
    "\n",
    "    Args:\n",
    "        iterable:   iterable to split\n",
    "        n:          number of pieces\n",
    "        collector:  split holder\n",
    "    Returns:\n",
    "        split iterator\n",
    "    \"\"\"\n",
    "\n",
    "    container = collector(iterable)\n",
    "    split_size, remainder = divmod(len(container), n)\n",
    "    n_elements = [split_size] * n\n",
    "    for i in range(remainder):\n",
    "        n_elements[i] += 1\n",
    "\n",
    "    first = last = 0\n",
    "    for n in n_elements:\n",
    "        last += n\n",
    "        yield container[first:last]\n",
    "        first = last\n",
    "\n",
    "\n",
    "class MutableStr:\n",
    "    __slots__ = ('data',)\n",
    "\n",
    "    def __init__(self):\n",
    "        self.data = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>> Main functions >>>\n",
    "\n",
    "def _is_russian_sentence(sentence: str) -> bool:\n",
    "    \"\"\"Checks if the given sentence is in Russian.\n",
    "\n",
    "    Args:\n",
    "        sentence:  sentence of interest\n",
    "\n",
    "    Returns:\n",
    "        Result of checking\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return detect(sentence) == 'ru'\n",
    "    except LangDetectException:\n",
    "        return False\n",
    "\n",
    "\n",
    "def _is_russian_text(text: str, thresh: float) -> bool:\n",
    "    \"\"\"Checks if the given text is in Russian.\n",
    "\n",
    "    Comes to this conclusion if the length of Russian sentences\n",
    "    is at least 'thresh' of the length of the entire text.\n",
    "\n",
    "    Args:\n",
    "        text:    text of interest\n",
    "        thresh:  identification threshold\n",
    "\n",
    "    Returns:\n",
    "        Result of checking\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(text, language='russian')\n",
    "\n",
    "    total_sent_len = total_ru_sent_len = 0\n",
    "    for sent in sentences:\n",
    "        sent_len = sum(not ch.isspace() for ch in sent)\n",
    "        total_sent_len += sent_len\n",
    "        if _is_russian_sentence(sent):\n",
    "            total_ru_sent_len += sent_len\n",
    "\n",
    "    try:\n",
    "        return (total_ru_sent_len / total_sent_len) >= thresh\n",
    "    except ZeroDivisionError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def _predict_for_files(file_names: Iterable[Union[PathLike, str]], thresh: float) -> str:\n",
    "    \"\"\"Takes file names and for each file determines whether it contains text in Russian.\n",
    "\n",
    "    Comes to the conclusion that the file contains Russian text\n",
    "    if the length of Russian sentences is at least 'thresh' of the length of the entire text.\n",
    "\n",
    "    Args:\n",
    "        file_names:  paths to files of interest\n",
    "        thresh:      identification threshold. Should be from [0, 1]\n",
    "\n",
    "    Returns:\n",
    "        CSV string containing entries of 'filename;answer\\n' format. Answer can be 0 or 1\n",
    "    \"\"\"\n",
    "    answers = ''\n",
    "\n",
    "    for file_name in file_names:\n",
    "        with open(file_name, 'r') as file:\n",
    "            content = file.read()\n",
    "        ru = _is_russian_text(content, thresh)\n",
    "        answers += f\"{basename(file_name)};{'1' if ru else '0'}\\n\"\n",
    "\n",
    "    return answers\n",
    "\n",
    "\n",
    "def predict(path: Union[PathLike, str],\n",
    "            *,\n",
    "            n_jobs: int = -1,\n",
    "            thresh: float = 0.1) -> None:\n",
    "    \"\"\"Takes a path to the folder and for each file\n",
    "    with the .txt extension in it determines whether it contains text in Russian.\n",
    "\n",
    "    Comes to the conclusion that the file contains Russian text\n",
    "    if the length of Russian sentences is at least 'thresh' of the length of the entire text.\n",
    "\n",
    "    Args:\n",
    "        path:    path to the folder of interest\n",
    "        n_jobs:  number of jobs to run in parallel. Must be positive or -1 (use all CPUs)\n",
    "        thresh:  identification threshold. Should be from [0, 1]\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if not 0 <= thresh <= 1:\n",
    "        raise ValueError(\"Parameter 'thresh' should be from [0, 1]\")\n",
    "    if n_jobs == -1:\n",
    "        n_jobs = cpu_count()\n",
    "\n",
    "    answers = MutableStr()\n",
    "    def callback(one_proc_answers: str) -> None:\n",
    "        answers.data += one_proc_answers\n",
    "\n",
    "    with Pool(n_jobs) as task_pool:\n",
    "        file_name_iter = iglob(join_path(path, '*.txt'))\n",
    "        for file_name_chunk in split(file_name_iter, n_jobs):\n",
    "            if not file_name_chunk:\n",
    "                break\n",
    "            task_pool.apply_async(\n",
    "                _predict_for_files,\n",
    "                (file_name_chunk, thresh),\n",
    "                callback=callback\n",
    "            )\n",
    "        task_pool.close()\n",
    "        task_pool.join()\n",
    "\n",
    "    with open(join_path(path, 'prediction.csv'), 'w') as ans_file:\n",
    "        ans_file.write(answers.data)\n",
    "\n",
    "\n",
    "def predict_once(text: str, *, thresh: float = 0.1) -> None:\n",
    "    \"\"\"Takes a string and determines whether it contains Russian text,\n",
    "    printing the result to the standard output (1 - yes, 0 - no).\n",
    "\n",
    "    Comes to this conclusion if the length of Russian sentences\n",
    "    is at least 'thresh' of the length of the entire text.\n",
    "\n",
    "    Args:\n",
    "        text:    text of iterest\n",
    "        thresh:  identification threshold. Should be from [0, 1]\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if not 0 <= thresh <= 1:\n",
    "        raise ValueError(\"Parameter 'thresh' should be from [0, 1]\")\n",
    "    print('1' if _is_russian_text(text, thresh) else '0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 126 ms, sys: 246 ms, total: 373 ms\n",
      "Wall time: 12.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "DATA_DIR = Path('data')\n",
    "\n",
    "predict(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "\n",
      "Ground truth\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "def test_predict_once():\n",
    "    print('Result')\n",
    "\n",
    "    predict_once(\n",
    "    \"\"\"En la sede del Consejo Federal de Inversiones se realizó el lanzamiento de la octava edición de la Noche de las Casas de Provincias, que tendrá lugar el próximo jueves 28 de 17 a 21 en todas las representaciones oficiales en Buenos Aires.\n",
    "    El secretario Bernardo Abruzzese participó del evento, que es organizado cada año por el Consejo Federal de Representaciones Oficiales (Confedro), organismo que nuclea a las casas de provincia, y que cuenta con el apoyo de diferentes organismos nacionales y gubernamentales, entre los que se destaca el Consejo Federal de Inversiones (CFI).\n",
    "    Este año, nuestra provincia contará con una variada propuesta cultural y turística, en la que se destaca la realización de la Marcha de los Bombos por el microcentro porteño, con la participación del “Indio” Froilán González y Tere Castronuovo, un homenaje a Vitillo Ábalos, y la presencia del Dúo Coplanacu como artista central.\n",
    "    Bajo la consigna “Recorré tu país en una noche”, todos los años las representaciones oficiales de las provincias en Buenos Aires abren sus puertas para promocionar y difundir cada una de las provincias desde lo turístico, cultural, su producción, etc.\n",
    "    Propuestas gastronómicas, degustación de productos, shows tecnológicos, artesanías, danza, muestras artísticas, sorteos, juegos, actividades para los más chicos, música en vivo y stands de productos regionales reciben a las miles de personas que se acercan a las Casas a conocer y disfrutar una fantástica noche federal para toda la familia.\n",
    "    Cuenta con el apoyo del Ministerio del Interior, el Gobierno de la Ciudad de Buenos Aires, la Cámara Argentina de Turismo y la Federación Argentina de Asociaciones de Empresas de Viajes y Turismo.\"\"\"\n",
    "    )\n",
    "\n",
    "    predict_once(\n",
    "    \"\"\"Министерство культуры планирует ужесточить правила охраны музеев, использовав опыт аэропортов. Об этом пишет «Интерфакс» со ссылкой на директора музейного департамента министерства Владислава Кононова.\n",
    "    Кононов заявил, что Минкультуры планирует внедрить на входе в музеи «систему выявления неадекватных посетителей». По его словам, музеи находятся на «передовой линии общественного внимания», и в связи с этим в них бывают «случаи девиантного поведения отдельных лиц».\n",
    "    Директор музейного департамента сослался на опыт аэропортов, говоря о необходимости противостоять непредсказуемым людям в музеях. В аэропортах, отметил он, наблюдение незаметно, но если человек привлечет внимание, к нему подойдут, проверят документы и спросят о самочувствии. То же самое необходимо внедрять в музеях, сказал Кононов. «Шедевры требуют к себе особого отношения в плане безопасности», — отметил он.\n",
    "    В январе 2019 года из Третьяковской галереи украли картину Архипа Куинджи «Ай-Петри. Крым» из коллекции Русского музея. До этого мужчина повредил в Третьяковской галерее картину Репина «Иван Грозный и сын его Иван». Ущерб оценили в 20 миллионов рублей.\"\"\"\n",
    "    )\n",
    "\n",
    "    predict_once(\n",
    "        'Саркофаг украшен фигурными рельефами, а также рельефами, несущими функцию декоративного орнамента'\n",
    "    )\n",
    "\n",
    "    predict_once(\n",
    "        'Арифметиканың барлыҡҡа килеүенең сәбәбе булып иҫәпләүҙәргә һәм ауыл хужалығын үҙәкләштергәндәге мәсьәләләр менән бәйле иҫәп-хисапҡа практик мохтажлыҡ тора.Фән хәл итеүҙе талап иткән мәсьәләләр менән бергә үҫешә. Арифметиканың үҫешенә грек математиктары ҙур өлөш индерә — атап әйткәндә, философтар-һандар ярҙамында донъяның бөтә законлыҡтарын аңларға һәм тасуирларға тырышыусы пифагорсылар.'\n",
    "    )\n",
    "\n",
    "    print('\\nGround truth', '0', '1', '1', '0', sep='\\n')\n",
    "\n",
    "test_predict_once()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что для распространённых языков всё работает хорошо.\n",
    "\n",
    "Но наш классификатор ошибочно определяет башкирский язык как русский, что неудивительно для наивного байесовского классификатора, анализирующего символьные n-граммы, ведь башкирский язык также использует кириллицу, но в обучении не встречался.\n",
    "\n",
    "Давайте поможем нашему классификатору отличать русский язык от других языков, использующих кириллицу. Для этого выделим наиболее распространённые слова русского языка.\n",
    "\n",
    "## Загрузка и обработка корпуса русской википедии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DUMP_DIR = Path('dumps')\n",
    "RU_WORD_FREQ_DUMP = DUMP_DIR / 'ru_word_freqs.pkl'\n",
    "\n",
    "if not isfile(RU_WORD_FREQ_DUMP):\n",
    "    # Загрузка предварительно сохранённого дампа результата обработки корпуса (if-ом ниже)\n",
    "    makedirs(DUMP_DIR, exist_ok=True)\n",
    "\n",
    "    RU_WORD_FREQ_DUMP_BZ2 = f'{RU_WORD_FREQ_DUMP}.bz2'\n",
    "    if not isfile(RU_WORD_FREQ_DUMP_BZ2):\n",
    "        download_file_from_google_drive('100vQH23cMAVa7Ii3ngQ4zRHySY7LWY47', RU_WORD_FREQ_DUMP_BZ2)\n",
    "    !pbzip2 -dk $RU_WORD_FREQ_DUMP_BZ2 2> /dev/null || bzip2 -dk $RU_WORD_FREQ_DUMP_BZ2\n",
    "    del RU_WORD_FREQ_DUMP_BZ2\n",
    "\n",
    "\n",
    "if getsize(RU_WORD_FREQ_DUMP) < 1_000_000:\n",
    "    # Уберите это условие, если хотите повторить обработку корпуса\n",
    "    # !!! REQUIRES MORE THAN 50GB\n",
    "\n",
    "    !wget https://zenodo.org/record/3827903/files/wikipedia-ru-2018.txt.gz -O dumps/wikipedia-ru-2018.txt.gz\n",
    "    !gzip -d dumps/wikipedia-ru-2018.txt.gz\n",
    "\n",
    "    with open(DUMP_DIR / 'wikipedia-ru-2018.txt', 'r') as wiki_dump_file:\n",
    "        wiki_dump = wiki_dump_file.read()\n",
    "    wiki_dump = wiki_dump.lower()\n",
    "\n",
    "    ru_word_freqs = Counter(\n",
    "        tok for tok in WordPunctTokenizer().tokenize(wiki_dump)\n",
    "        if 'А' <= tok[0] <= 'я'  # Getting rid of non-word and non-Russian tokens\n",
    "    )\n",
    "    del wiki_dump\n",
    "    with open(RU_WORD_FREQ_DUMP, 'wb') as pkl:\n",
    "        dump(ru_word_freqs, pkl)\n",
    "else:\n",
    "    with open(RU_WORD_FREQ_DUMP, 'rb') as pkl:\n",
    "        ru_word_freqs = load(pkl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраним 200_000 наиболее распространённых слов во множество и модифицируем функцию `_is_russian_sentence`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ru_most_common = frozenset(map(itemgetter(0), ru_word_freqs.most_common(200_000)))\n",
    "\n",
    "\n",
    "def _is_russian_sentence(sentence: str, *, tokenizer=WordPunctTokenizer()) -> bool:\n",
    "    \"\"\"Checks if the given sentence is in Russian.\n",
    "\n",
    "    Args:\n",
    "        sentence:   sentence of interest\n",
    "        tokenizer:  word tokenizer\n",
    "\n",
    "    Returns:\n",
    "        Result of checking\n",
    "    \"\"\"\n",
    "    try:\n",
    "        res = detect(sentence)\n",
    "    except LangDetectException:\n",
    "        return False\n",
    "    if res == 'ru':\n",
    "        words = tuple(\n",
    "            tok for tok in tokenizer.tokenize(sentence.lower())\n",
    "            if tok[0].isidentifier()  # Getting rid of non-word tokens\n",
    "        )\n",
    "        n_rus_words = sum(\n",
    "            1 for word in words\n",
    "            if word in ru_most_common  # or all('А' <= ch <= 'я' for ch in word)\n",
    "        )\n",
    "        try:\n",
    "            return (n_rus_words / len(words)) >= 0.8  # Hard coded constant!\n",
    "        except ZeroDivisionError:\n",
    "            pass\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "\n",
      "Ground truth\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "test_predict_once()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Замечательно. Башкирский язык теперь определяется правильно.\n",
    "\n",
    "Давайте теперь вручную проаннотируем некоторое подможество текстов из предоставленной Вами выборки. Это необходимо для более надёжной оценки качества модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual annotation result\n",
    "is_russian_truth = {\n",
    "    '1000.txt': False,\n",
    "    '1001.txt': False,\n",
    "    '1002.txt': False,\n",
    "    '1003.txt': False,\n",
    "    '1004.txt': False,\n",
    "    '1005.txt': False,\n",
    "    '1006.txt': False,\n",
    "    '1007.txt': False,\n",
    "    '1008.txt': False,\n",
    "    '1009.txt': False,\n",
    "    '1010.txt': False,\n",
    "    '1011.txt': False,\n",
    "    '1012.txt': False,\n",
    "    '1013.txt': False,\n",
    "    '1014.txt': False,\n",
    "    '1015.txt': True,\n",
    "    '1016.txt': False,\n",
    "    '1017.txt': False,\n",
    "    '1018.txt': False,\n",
    "    '1019.txt': False,\n",
    "    '1021.txt': False,\n",
    "    '1022.txt': False,\n",
    "    '1023.txt': True,\n",
    "    '1024.txt': False,\n",
    "    '1025.txt': False,\n",
    "    '1026.txt': True,\n",
    "    '1027.txt': False,\n",
    "    '1028.txt': False,\n",
    "    '1029.txt': False,\n",
    "    '1030.txt': False,\n",
    "    '1031.txt': False,\n",
    "    '1032.txt': False,\n",
    "    '1033.txt': False,\n",
    "    '1034.txt': False,\n",
    "    '1035.txt': False,\n",
    "    '1036.txt': False,\n",
    "    '1037.txt': False,\n",
    "    '1038.txt': True,\n",
    "    '1039.txt': False,\n",
    "    '1040.txt': False,\n",
    "    '1041.txt': False,\n",
    "    '1042.txt': False,\n",
    "    '1043.txt': False,\n",
    "    '1045.txt': False,\n",
    "    '1046.txt': False,\n",
    "    '1047.txt': False,\n",
    "    '1048.txt': False,\n",
    "    '1049.txt': False,\n",
    "    '1050.txt': False,\n",
    "    '1051.txt': False,\n",
    "    '1052.txt': False,\n",
    "    '1054.txt': False,\n",
    "    '1055.txt': False,\n",
    "    '1056.txt': False,\n",
    "    '1057.txt': False,\n",
    "    '1058.txt': False,\n",
    "    '1060.txt': False,\n",
    "    '1061.txt': True,\n",
    "    '1062.txt': False,\n",
    "    '1063.txt': False,\n",
    "    '1064.txt': False,\n",
    "    '1065.txt': False,\n",
    "    '1066.txt': False,\n",
    "    '1067.txt': False,\n",
    "    '1068.txt': False,\n",
    "    '1069.txt': False,\n",
    "    '1070.txt': False,\n",
    "    '1071.txt': False,\n",
    "    '1072.txt': False,\n",
    "    '1073.txt': False,\n",
    "    '1074.txt': False,\n",
    "    '1075.txt': False,\n",
    "    '1076.txt': False,\n",
    "    '1077.txt': False,\n",
    "    '1078.txt': False,\n",
    "    '1079.txt': False,\n",
    "    '1080.txt': False,\n",
    "    '1081.txt': False,\n",
    "    '1082.txt': False,\n",
    "    '1083.txt': False,\n",
    "    '1084.txt': False,\n",
    "    '1085.txt': False,\n",
    "    '1086.txt': False,\n",
    "    '1087.txt': False,\n",
    "    '1088.txt': False,\n",
    "    '1089.txt': True,\n",
    "    '1090.txt': False,\n",
    "    '1091.txt': True,\n",
    "    '1092.txt': False,\n",
    "    '1093.txt': False,\n",
    "    '1094.txt': True,\n",
    "    '1095.txt': False,\n",
    "    '1096.txt': False,\n",
    "    '1097.txt': True,\n",
    "    '1098.txt': False,\n",
    "    '1099.txt': False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вызовем функцию `predict` ещё раз и выведем в стандартный поток те имена файлов из подвыборки, класс которых был предсказан неправильно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(DATA_DIR)\n",
    "\n",
    "with open(DATA_DIR / 'prediction.csv', 'r') as file:\n",
    "    is_russian_pred = {\n",
    "        file: ru == '1'\n",
    "        for file, ru in csv_reader(file, delimiter=';')\n",
    "    }\n",
    "\n",
    "for file_basename, ru_ground_truth in is_russian_truth.items():\n",
    "    if is_russian_pred[file_basename] != ru_ground_truth:\n",
    "        print(file_basename, ru_ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таковых не нашлось, что может объясняться малым объёмом тестовой выборки.\n",
    "\n",
    "Как бы то ни было, можно видеть, что данный классификатор поставленную задачу выполняет успешно."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
